{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T12:43:19.853734Z",
     "start_time": "2022-06-29T12:43:19.834746Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T12:43:20.616736Z",
     "start_time": "2022-06-29T12:43:20.601747Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = \"../dataset\"\n",
    "preprocessed_dataset_path = \"../dataset/preprocessed_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T12:43:32.411682Z",
     "start_time": "2022-06-29T12:43:21.081749Z"
    }
   },
   "outputs": [],
   "source": [
    "from codes.docs.analysis import data_preprocessing, genetic_file_preprocess\n",
    "from codes.docs.analysis.nimagen import genes, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T12:43:38.277874Z",
     "start_time": "2022-06-29T12:43:38.033715Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook 1 description: \n",
    "1. Imaging data is imported and preprocessed\n",
    "\n",
    "Available informations\n",
    "- Volumetric dataset (dHCP) - 675 individuals / release 2\n",
    "- Diffusion dataset - 432 individuals\n",
    "- Microstructure - 318 individuals\n",
    "- Cortical thickness - 234 individuals\n",
    "2. Genetic data is added and outliers removed\n",
    "- Schizophrenia - PRS calculated from GWAS 2020 (Ripke et al., 2020)\n",
    "- ASD - PRS calculated from GWAS 2019 (Grove et al, 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing and Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Importing volumetric, diffusion, microstructural, cortical thickness dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dHCP_release3_metadata.tsv` is downloaded from [here](https://biomedia.github.io/dHCP-release-notes/download.html). Total rows = 887\n",
    "\n",
    "saved in `dataset/participant_info/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dHCP_metadata = pd.read_csv(f'{dataset_path}/participant_info/dHCP_release3_metadata.tsv',sep='\\t')\n",
    "dHCP_metadata.rename(columns={'participant_id':'ID',\n",
    "                     'session_id':'Session',\n",
    "                     'birth_age':'GA',\n",
    "                     'scan_age':'PMA'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(887, 39)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dHCP_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volumetric dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `volumetric_dataset` contains all the volumetric information for the full cohort. Labels are saved in `Imperial_label` and `AAL_label`.\n",
    "\n",
    "* AAL is derived by Dafnis.\n",
    "* DrawEM (preferred method) is part of dHCP pipeline.\n",
    "\n",
    "Volume are calculated directly from segmentation, where for each label, number of voxel is multiplied by the voxel dimension\n",
    "\n",
    "`for_each drawem_dseg_FOLDER fslstats -K drawem_dseg.nii.gz drawem_dseg.nii.gz -V`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following files are available (saved in dataset/volumetric/full_dataset/)\n",
    "\n",
    "* `allSubjectsWithKANA_AALVolumes_mm3_4Harriet_withCovariates_withImperialLegendsOK.xlsx` created by Dafnis (likely neonatal release 2). The last subject ID is 1074. Total rows = 750. Contains AAL and DrawEM (750 includes repeated measures)\n",
    "* `neonatal_release3_DrawEM_Volume_Hai.tsv` created by Hai using segementation files in dhcp-pipeline-data/BIDS_public/rel3_dhcp_anat_pipeline/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### neonatal release 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Volumetric dataset neonatal release 2\n",
    "volumetric_dataset = pd.read_excel(open(\n",
    "    '../dataset/volumetric/full_dataset/allSubjectsWithKANA_AALVolumes_mm3_4Harriet_withCovariates_withImperialLegendsOK.xlsx',\n",
    "    'rb'),\n",
    "                             sheet_name='allSubjectsWithKANA_AALVolumes').drop(\n",
    "                                 'ID', axis=1)\n",
    "volumetric_dataset = volumetric_dataset.rename({'Subject': 'ID'}, axis=1)\n",
    "volumetric_dataset.rename(columns={\n",
    "    'GA at birth': 'GA',\n",
    "    'PMA at birth': 'PMA'\n",
    "},\n",
    "                    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Imperial_label = pd.read_excel(open(\n",
    "    f'{dataset_path}/volumetric/full_dataset/allSubjectsWithKANA_AALVolumes_mm3_4Harriet_withCovariates_withImperialLegendsOK.xlsx',\n",
    "    'rb'),\n",
    "                               sheet_name='IMPERIAL ROIs LEGEND',\n",
    "                               header=None).drop([0, 1], axis=1)\n",
    "AAL_label = pd.read_excel(open(\n",
    "    f'{dataset_path}/volumetric/full_dataset/allSubjectsWithKANA_AALVolumes_mm3_4Harriet_withCovariates_withImperialLegendsOK.xlsx',\n",
    "    'rb'),\n",
    "                          sheet_name='AAL LEGEND',\n",
    "                          header=None).drop(0, axis=1)\n",
    "\n",
    "from string import digits\n",
    "\n",
    "AAL_label = np.asarray(\n",
    "    [str.split(i, ' ')[1] for i in np.asarray(AAL_label).reshape(-1)])\n",
    "Imperial_label = np.asarray([\n",
    "    str.strip(i.translate({ord(c): None\n",
    "                           for c in digits}))\n",
    "    for i in (np.asarray(Imperial_label)).reshape(-1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volumetric_dataset = volumetric_dataset.drop(columns=['GA','PMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(750, 199)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('volumetric dataset')\n",
    "volumetric_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### neonatal release 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 885 individuals with structural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volumetric_dataset = pd.read_table('../dataset/volumetric/full_dataset/neonatal_release3_DrawEM_Volume_Hai.tsv',delim_whitespace=True,header=None)\n",
    "volumetric_dataset.columns = ['ID/ses'] + [f'Imperial {i}' for i in range(1,88)]\n",
    "volumetric_dataset['ID'] = [i.split('/')[0].replace('sub-','') for i in volumetric_dataset['ID/ses']]\n",
    "volumetric_dataset['Session'] = [i.split('/')[1].replace('ses-','') for i in volumetric_dataset['ID/ses']]\n",
    "volumetric_dataset['Session'] = volumetric_dataset['Session'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volumetric_dataset = volumetric_dataset[['ID','Session']+[f'Imperial {i}' for i in range(1,88)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volume_9TT_dataset = pd.read_table('../dataset/volumetric/full_dataset/9TT.txt',delim_whitespace=True,header=None)\n",
    "volume_9TT_dataset.columns = ['ID/ses'] + ['CSF','GM','WM','Background','Ventricles','Cerebellum','deepGM','Brainstem','Hipp+Amygdala']\n",
    "volume_9TT_dataset['ID'] = [i.split('/')[0].replace('sub-','') for i in volume_9TT_dataset['ID/ses']]\n",
    "volume_9TT_dataset['Session'] = [i.split('/')[1].replace('ses-','') for i in volume_9TT_dataset['ID/ses']]\n",
    "volume_9TT_dataset['Session'] = volume_9TT_dataset['Session'].astype('int64')\n",
    "volume_9TT_dataset['TBV']=volume_9TT_dataset[['WM','GM','Cerebellum','deepGM','Brainstem','Hipp+Amygdala']].sum(axis=1)\n",
    "volume_9TT_dataset['ICV']=volume_9TT_dataset[['CSF','WM','GM','Ventricles','Cerebellum','deepGM','Brainstem','Hipp+Amygdala']].sum(axis=1)\n",
    "volume_9TT_dataset = volume_9TT_dataset[['ID','Session']+['TBV','ICV','CSF','GM','WM','Background','Ventricles','Cerebellum','deepGM','Brainstem','Hipp+Amygdala']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volumetric_dataset = pd.merge(volume_9TT_dataset,volumetric_dataset,on=['ID','Session'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "volumetric_dataset.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/volumetric_dataset_neonatal_release_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumetric_dataset = pd.read_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/volumetric_dataset_neonatal_release_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Diffusion dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The diffusion dataset is saved in the matlab file. \n",
    "It is saved in the `diffusion_full_dataset`\n",
    "\n",
    "The labels are saved in the `ROIs_combinations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ROIs_combinations = data_preprocessing.Diffusion.create_ROIs_combinations(\n",
    "    f'../../dataset/diff_dataset/Regions_of_Interests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Diffusion dataset\n",
    "diffusion_full_dataset = scipy.io.loadmat(\n",
    "    f'../../dataset/diff_dataset/2020_04_MASTER_connectomes90_433subj_SCandFC_TermScansVolOK.mat'\n",
    ")\n",
    "diffusion_matrices = diffusion_full_dataset[\n",
    "    'SCmu']  #use the Structural Connectivity (SIFT2-weighted * Mu) [equivalent to raw connectivity, see SIFT2 paper]\n",
    "diffusion_matrices = [\n",
    "    diffusion_matrices[:, :, i] for i in range(diffusion_matrices.shape[2])\n",
    "]  # rearranged them into (433,90,90)\n",
    "diffusion_matrices = np.asarray(diffusion_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diffusion_matrices = np.asarray([\n",
    "    data_preprocessing.Diffusion.lower_triangle(diffusion_matrices[i])\n",
    "    for i in range(diffusion_matrices.shape[0])\n",
    "])\n",
    "diffusion_matrices = pd.DataFrame(diffusion_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diffusion_matrices.columns = data_preprocessing.Diffusion.lower_triangle(\n",
    "    ROIs_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diffusion_subject_id = diffusion_full_dataset['sub'].reshape(-1, 1)\n",
    "diffusion_subject_id = [\n",
    "    i[0][0].replace('sub-', '') for i in diffusion_subject_id\n",
    "]\n",
    "diffusion_subject_sess = diffusion_full_dataset['ses'].reshape(-1, 1)\n",
    "diffusion_subject_sess = [\n",
    "    i[0][0].replace('ses-', '') for i in diffusion_subject_sess\n",
    "]\n",
    "diffusion_subject_pma = diffusion_full_dataset['pma'].reshape(-1)\n",
    "diffusion_subject_ga = diffusion_full_dataset['ga'].reshape(-1)\n",
    "\n",
    "diffusion_dataset = pd.DataFrame({\n",
    "    'ID': diffusion_subject_id,\n",
    "    'Session': diffusion_subject_sess,\n",
    "    'GA': diffusion_subject_ga,\n",
    "    'PMA': diffusion_subject_pma\n",
    "})\n",
    "\n",
    "# diffusion_dataset['Gender'] = diffusion_full_dataset['sex'].reshape(-1)\n",
    "\n",
    "#match the ID to the diffusion matrices\n",
    "diffusion_dataset = pd.concat((diffusion_dataset, diffusion_matrices), axis=1)\n",
    "diffusion_dataset['Session'] = diffusion_dataset['Session'].astype('int64')\n",
    "#rearrange by the session, drop the duplicates and keep the last session\n",
    "# diffusion_dataset = diffusion_dataset.sort_values(by='Session',\n",
    "#                                                   ascending=False)\n",
    "# diffusion_dataset = diffusion_dataset.drop_duplicates(subset='ID', keep='last')\n",
    "# diffusion_dataset = diffusion_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusion dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(433, 4009)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('diffusion dataset')\n",
    "diffusion_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "diffusion_dataset.to_csv(\n",
    "    f'{preprocessed_dataset_path}/imaging_data/log_file/diffusion_dataset.csv',\n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### microstructure dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "microstructure data contains `FA`, `MD`, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Importing the microstructure files\n",
    "micro_dict_europeans_df = defaultdict(pd.DataFrame)\n",
    "for file in glob.glob(microstructure_file_path +\n",
    "                      '/europeans/*-wm-*-stats*.csv'):\n",
    "    micro = str.split(file, '-')[-3]\n",
    "    micro_dict_europeans_df[micro] = pd.read_csv(file).dropna(how='any',\n",
    "                                                              axis=1)\n",
    "    micro_dict_europeans_df[micro].columns = [\n",
    "        i + str.upper('_' + micro) if i != 'id' else 'ID'\n",
    "        for i in micro_dict_europeans_df[micro].columns\n",
    "    ]  # adding the microstructure name to each of the feature\n",
    "    micro_dict_europeans_df[micro]['ID'] = [\n",
    "        i.split('-')[2].split('_')[0]\n",
    "        for i in micro_dict_europeans_df[micro]['ID']\n",
    "    ]  # change the ID format\n",
    "\n",
    "# Adding the microstructure files together into one table.\n",
    "from functools import reduce\n",
    "\n",
    "dHCP_microstructure_europeans = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on='ID', how='outer'), [\n",
    "        micro_dict_europeans_df['fa'], micro_dict_europeans_df['md'],\n",
    "        micro_dict_europeans_df['t12'], micro_dict_europeans_df['t2'],\n",
    "        micro_dict_europeans_df['fiso']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#### adding asians microstructure files\n",
    "micro_dict_asians_df = defaultdict(pd.DataFrame)\n",
    "for file in glob.glob(microstructure_file_path + '/asians/*-wm-*-stats*.csv'):\n",
    "    micro = str.split(file, '-')[-3]\n",
    "    micro_dict_asians_df[micro] = pd.read_csv(file).dropna(how='any', axis=1)\n",
    "    micro_dict_asians_df[micro].columns = [\n",
    "        i + str.upper('_' + micro) if i != 'id' else 'ID'\n",
    "        for i in micro_dict_asians_df[micro].columns\n",
    "    ]  # adding the microstructure name to each of the feature\n",
    "    micro_dict_asians_df[micro]['ID'] = [\n",
    "        i.split('-')[2].split('_')[0]\n",
    "        for i in micro_dict_asians_df[micro]['ID']\n",
    "    ]  # change the ID format\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "dHCP_microstructure_asians = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on='ID', how='outer'), [\n",
    "        micro_dict_asians_df['fa'], micro_dict_asians_df['md'],\n",
    "        micro_dict_asians_df['t12'], micro_dict_asians_df['t2'],\n",
    "        micro_dict_asians_df['fiso']\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dHCP_microstructure = pd.concat(\n",
    "    [dHCP_microstructure_europeans,\n",
    "     dHCP_microstructure_asians]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microstructure info\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(318, 276)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('microstructure info')\n",
    "dHCP_microstructure_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dHCP_microstructure.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/microstructure_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Cortical Thickness dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`cortical_thickness_df` contains the mean cortical thickness information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cortical_thickness_df = pd.read_csv('../../dataset/cortical_thickness/cortical_thickness_updated.csv')\n",
    "cortical_thickness_df.rename(columns={'session':'Session'},inplace=True)\n",
    "# cortical_thickness_df.drop('session', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Session</th>\n",
       "      <th>mean_CC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC00328XX15</td>\n",
       "      <td>104800</td>\n",
       "      <td>1.609608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC00540XX13</td>\n",
       "      <td>164400</td>\n",
       "      <td>1.662574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC00882XX17</td>\n",
       "      <td>13030</td>\n",
       "      <td>1.622470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC00257XX10</td>\n",
       "      <td>84700</td>\n",
       "      <td>1.533126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC00286XX15</td>\n",
       "      <td>91700</td>\n",
       "      <td>1.788949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>CC00217XX11</td>\n",
       "      <td>73700</td>\n",
       "      <td>1.646990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>CC00255XX08</td>\n",
       "      <td>84400</td>\n",
       "      <td>1.859827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>CC00675XX16</td>\n",
       "      <td>219100</td>\n",
       "      <td>1.496696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>CC00247XX17</td>\n",
       "      <td>82801</td>\n",
       "      <td>1.617542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>CC00325XX12</td>\n",
       "      <td>103900</td>\n",
       "      <td>1.585910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Session   mean_CC\n",
       "0    CC00328XX15   104800  1.609608\n",
       "1    CC00540XX13   164400  1.662574\n",
       "2    CC00882XX17    13030  1.622470\n",
       "3    CC00257XX10    84700  1.533126\n",
       "4    CC00286XX15    91700  1.788949\n",
       "..           ...      ...       ...\n",
       "229  CC00217XX11    73700  1.646990\n",
       "230  CC00255XX08    84400  1.859827\n",
       "231  CC00675XX16   219100  1.496696\n",
       "232  CC00247XX17    82801  1.617542\n",
       "233  CC00325XX12   103900  1.585910\n",
       "\n",
       "[234 rows x 3 columns]"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cortical_thickness_df['Session']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cortical_thickness_df.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/cortical_thickness_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DWI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../codes/FBA/generate_ID_list.py generate --folder ../codes/FBA/data/ --pattern \"sub*/ses*\" --duplicates | python ../codes/FBA/generate_ID_list.py generate --folder ../codes/FBA/dhcp_neo_dMRI_derived/ --pattern \"sub*/ses*\" --no-duplicates | tail -n +3 | tr '/' ',' > ../codes/FBA/available_term_preterm.txt\n",
    "# ! tr '/' ',' < ../codes/FBA/available_term_preterm.txt > ../codes/FBA/available_term_preterm.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../codes/FBA/generate_ID_list.py generate --folder ../codes/FBA/data/ --pattern \"sub*/ses*\" --duplicates | python ../codes/FBA/generate_ID_list.py generate --folder ../codes/FBA/dhcp_neo_dMRI_derived/ --pattern \"sub*/ses*\" --duplicates | tail -n +3 | tr '/' ',' > ../codes/FBA/available_dwi.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_DWI = pd.read_csv('../codes/FBA/available_dwi.txt',sep=',',header=None,names=['ID','Session']) # available dwi AND warps (registered to 40wks template)\n",
    "available_DWI['ID'] =  available_DWI['ID'].str.replace('sub-','')\n",
    "available_DWI['Session'] = available_DWI['Session'].str.replace('ses-','').astype('int64')\n",
    "available_DWI['available_dmri'] = 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "797 have dmri, but 745 are registered to the template (40 weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Session</th>\n",
       "      <th>available_dmri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC00830XX14</td>\n",
       "      <td>18910</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC00830XX14</td>\n",
       "      <td>30710</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC00792XX18</td>\n",
       "      <td>244200</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CC00792XX18</td>\n",
       "      <td>1800</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC00094AN13</td>\n",
       "      <td>33500</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>CC00845AN21</td>\n",
       "      <td>32010</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>CC00945BN22</td>\n",
       "      <td>14330</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>CC00415XX11</td>\n",
       "      <td>127400</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>CC00499XX22</td>\n",
       "      <td>145800</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>CC00065XX08</td>\n",
       "      <td>18600</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>745 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Session available_dmri\n",
       "0    CC00830XX14    18910            yes\n",
       "1    CC00830XX14    30710            yes\n",
       "2    CC00792XX18   244200            yes\n",
       "3    CC00792XX18     1800            yes\n",
       "4    CC00094AN13    33500            yes\n",
       "..           ...      ...            ...\n",
       "740  CC00845AN21    32010            yes\n",
       "741  CC00945BN22    14330            yes\n",
       "742  CC00415XX11   127400            yes\n",
       "743  CC00499XX22   145800            yes\n",
       "744  CC00065XX08    18600            yes\n",
       "\n",
       "[745 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_DWI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all data with PRS scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PRS file contains PRS at 11 P-value thresholds, which ranges from 1e-8 to 1.\n",
    "- The Ancestry files contains the first 10 PCs of the genetic data\n",
    "- The data can be divided into 3 different ethnic groups: European, South Asian and African\n",
    "- The outliers are removed to each cohort separately (i.e., European, European + South Asian (called mixed cohort), and European + South Asian + Africans (called full cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T12:43:44.772680Z",
     "start_time": "2022-06-29T12:43:43.697776Z"
    }
   },
   "outputs": [],
   "source": [
    "# volumetric_dataset = pd.read_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/volumetric_dataset_neonatal_release_3.csv')\n",
    "# cortical_thickness_df = pd.read_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/cortical_thickness_df.csv')\n",
    "# diffusion_dataset = pd.read_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/diffusion_dataset.csv')\n",
    "# microstructure_dataset = pd.read_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/microstructure_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "full_dataset = volumetric_dataset.merge(dHCP_metadata,on=['ID','Session'],how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset.merge(available_DWI,on=['ID','Session'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset = volumetric_dataset.merge(diffusion_dataset,\n",
    "#                                   on=['ID','Session'],\n",
    "#                                   suffixes=('_vol', '_diff'),\n",
    "#                                   how='outer')  #merging the diffusion dataset\n",
    "# full_dataset = full_dataset.merge(diffusion_dataset,\n",
    "#                                   on=['ID','Session'],\n",
    "#                                   suffixes=('_vol', '_diff'),\n",
    "#                                   how='outer')  #merging the diffusion dataset\n",
    "\n",
    "# full_dataset = full_dataset.merge(dHCP_metadata,on=['ID','Session'],how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "full_dataset.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/combined_imaging_full_dataset_release2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**run from here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/combined_imaging_full_dataset_release3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T12:43:51.392994Z",
     "start_time": "2022-06-29T12:43:49.681931Z"
    }
   },
   "outputs": [],
   "source": [
    "full_dataset = pd.read_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/combined_imaging_full_dataset_release3.csv',index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following files can be generated\n",
    "* rel2_batch1\n",
    "* rel2_batch2\n",
    "* rel3_batch1\n",
    "* rel3_batch2\n",
    "\n",
    "for either asd or genetics\n",
    "\n",
    "also depends on the type of LD you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch 1 genetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort = genetic_file_preprocess.Cohort(\n",
    "    cohort_name='euro',\n",
    "    Ancestry_file_path=\n",
    "    f'{dataset_path}/ancestry_PCs/file3_only_europ_pca.eigenvec',\n",
    "    imaging_df=full_dataset)\n",
    "\n",
    "mixed_cohort = genetic_file_preprocess.Cohort(\n",
    "    cohort_name='mixed',\n",
    "    Ancestry_file_path=\n",
    "    f'{dataset_path}/ancestry_PCs/euro_asian_361_file3_pca.eigenvec',\n",
    "    imaging_df=full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df = european_cohort.extract_neonates_data(columns=['Imperial 1','termness'],criteria={'termness':'preterm/term_at_scan'},remove_duplicates=True) # other options = preterm/term_at_scan\n",
    "mixed_cohort.volumetric_df = mixed_cohort.extract_neonates_data(columns=['Imperial 1','termness'],criteria={'termness':'preterm/term_at_scan'},remove_duplicates=True) # other options = preterm/term_at_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df = european_cohort.extract_neonates_data(columns=['Imperial 1','termness'],criteria={'termness':'term'},remove_duplicates=True) # other options = preterm/term_at_scan\n",
    "mixed_cohort.volumetric_df = mixed_cohort.extract_neonates_data(columns=['Imperial 1','termness'],criteria={'termness':'term'},remove_duplicates=True) # other options = preterm/term_at_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  batch 2 genetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort = genetic_file_preprocess.Cohort(\n",
    "    cohort_name='euro',\n",
    "    Ancestry_file_path=\n",
    "    f'{dataset_path}/ancestry_PCs/euro_batch2_genotyped_PCA_result.eigenvec',\n",
    "    imaging_df=full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df = european_cohort.extract_neonates_data(columns=['Imperial 1','termness'],criteria={'termness':'term'},remove_duplicates=True) # other options = preterm/term_at_scan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df = european_cohort.extract_neonates_data(columns=['Imperial 1','termness'],criteria={'termness':'preterm/term_at_scan'},remove_duplicates=True) # other options = preterm/term_at_scan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding genetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scz_prs_batch1_euro = genetic_file_preprocess.Cohort.preprocess_PRSice_PRS_Anc_files('../dataset/PRS/scz/SCZ3_lifted37_dHCP_merged_cleaned_EUROPEANS_updated.gwas.all.score',column_prefix='SCZ_PRS_') # SCZ all LDs batch 1\n",
    "\n",
    "scz_prs_batch2_euro = genetic_file_preprocess.Cohort.preprocess_PRSice_PRS_Anc_files('../dataset/PRS/scz/SCZ3core_nodups_euro_batch2_genotyped.gwas.all_score',column_prefix='SCZ_PRS_') # SCZ all LDs batch 1\n",
    "\n",
    "asd_prs_batch1_euro = genetic_file_preprocess.Cohort.preprocess_PRSice_PRS_Anc_files('../dataset/PRS/asd/ASD_eurold_lifted37_dHCP_merged_cleaned_EUROPEANS.gwas.all.score', column_prefix = 'ASD_PRS_') # ASD euro LDs batch 1\n",
    "\n",
    "asd_prs_batch2_euro = genetic_file_preprocess.Cohort.preprocess_PRSice_PRS_Anc_files('../dataset/PRS/asd/ASD_nodups_euro_batch2_genotyped.gwas.all_score',column_prefix='ASD_PRS_') # SCZ all LDs batch 1\n",
    "\n",
    "\n",
    "scz_prs_batch1_mixed = genetic_file_preprocess.Cohort.preprocess_PRSice_PRS_Anc_files('../dataset/PRS/scz/SCZ3_lifted37_dHCP_merged_cleaned_EUROPEANS_ASIAN_361.gwas.all.score',column_prefix='SCZ_PRS_') # SCZ all LDs batch 1\n",
    "\n",
    "asd_prs_batch1_mixed = genetic_file_preprocess.Cohort.preprocess_PRSice_PRS_Anc_files('../dataset/PRS/asd/ASD_eurold_lifted37_dHCP_merged_cleaned_EUROPEAN_ASIAN.gwas.all.score', column_prefix = 'ASD_PRS_') # ASD euro LDs batch 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df = european_cohort.volumetric_df.merge(scz_prs_batch1_euro,on='ID',how='inner')\n",
    "european_cohort.volumetric_df = european_cohort.volumetric_df.merge(asd_prs_batch1_euro,on='ID',how='inner')\n",
    "\n",
    "# mixed_cohort.volumetric_df = mixed_cohort.volumetric_df.merge(scz_prs_batch1_mixed,on='ID',how='inner')\n",
    "# mixed_cohort.volumetric_df = mixed_cohort.volumetric_df.merge(asd_prs_batch1_mixed,on='ID',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df = european_cohort.volumetric_df.merge(scz_prs_batch2_euro,on='ID',how='inner')\n",
    "european_cohort.volumetric_df = european_cohort.volumetric_df.merge(asd_prs_batch2_euro,on='ID',how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving preprocessed data without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/rel3/volumetric/european_volumetric_df_rel3_batch1.csv',index=False)\n",
    "\n",
    "mixed_cohort.volumetric_df.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/rel3/volumetric/mixed_volumetric_df_rel3_batch1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 359)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_cohort.volumetric_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/rel3/volumetric/european_volumetric_df_rel3_batch2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/rel2/volumetric/test_european_volumetric_df_rel2_batch2.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/rel2/volumetric/european_volumetric_df_rel2_batch1.csv',index=False)\n",
    "\n",
    "mixed_cohort.volumetric_df.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/rel2/volumetric/mixed_volumetric_df_rel2_batch1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "european_cohort.volumetric_df.to_csv(f'{preprocessed_dataset_path}/imaging_data/log_file/rel3/volumetric/european_volumetric_df_rel3_batch2_PRETERM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
